{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data with Reddit jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "cons_red_df = pl.read_csv(\"consice_reddit.csv\")\n",
    "print(cons_red_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the whole joke\n",
    "cons_red_df = cons_red_df.with_columns([\n",
    "    (pl.col(\"title\") + \"\\n\" + pl.col(\"selftext\")).alias(\"whole_joke\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cons_red_df[\"whole_joke\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let GPT-3.5 rate funnyness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not store API keys in text in prod environments :)\n",
    "openai_base_url = \"\"\n",
    "openai_api_key = \"\"\n",
    "\n",
    "# Initialize a client\n",
    "client = openai.OpenAI(api_key=openai_api_key, base_url=openai_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_for_rating_funniness = \"\"\"As a professional joke rater, you rate jokes from 0 (not funny at all), to 5 (died laughing). Only output the rating number.\n",
    "Please rate this joke:\\n\"\"\"\n",
    "\n",
    "def open_ai_gateway(prompt):\n",
    "    # Create a chat completion\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "        }],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content \n",
    "\n",
    "# Create a new column in the DataFrame to store the ratings\n",
    "cons_red_df = cons_red_df.with_columns([pl.lit(None).alias(\"funniness_rating\")])\n",
    "\n",
    "# Iterate over the DataFrame and get the ratings with a progress bar\n",
    "ratings = []\n",
    "for row in tqdm(cons_red_df.iter_rows(), total=cons_red_df.height, desc=\"Processing jokes\"):\n",
    "    curr_prompt = prompt_for_rating_funniness + row[cons_red_df.columns.index(\"whole_joke\")]\n",
    "    rating = open_ai_gateway(curr_prompt)\n",
    "    ratings.append(rating)\n",
    "\n",
    "cons_red_df = cons_red_df.with_columns([pl.Series(\"funniness_rating\", ratings)])\n",
    "\n",
    "print(cons_red_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let GPT explain the joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_explanation = \"\"\"As a professional joke explainer, please provide an explanation for this joke:\n",
    "\"\"\"\n",
    "\n",
    "# Create a new column in the DataFrame to store the explanations\n",
    "cons_red_df = cons_red_df.with_columns([pl.lit(None).alias(\"joke_explanation\")])\n",
    "\n",
    "# Iterate over the DataFrame and get the explanations with a progress bar\n",
    "explanations = []\n",
    "for row in tqdm(cons_red_df.iter_rows(), total=cons_red_df.height, desc=\"Processing jokes\"):\n",
    "    curr_prompt = prompt_for_explanation + row[cons_red_df.columns.index(\"whole_joke\")]\n",
    "    explanation = open_ai_gateway(curr_prompt)\n",
    "    explanations.append(explanation)\n",
    "\n",
    "cons_red_df = cons_red_df.with_columns([pl.Series(\"joke_explanation\", explanations)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cons_red_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display specific cell\n",
    "print(cons_red_df[2, -3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "\n",
    "Turn the above code into a Kedro pipeline! You can find the relevant code under `src/reddit_summarizer/pipelines/data_processing`. Start by initializing the data, by running `init_data.ipynb`.\n",
    "\n",
    "You can visually inspecting the pipeline with `kedro viz run --autoreload`. When ready to test pipelines, simply run `kedro run`.\n",
    "\n",
    "During your development process, it can be helpful to inspect what is saved in the different steps of the pipelines â€“ this can be done in the notebook under `notebooks/kedro_catalog.ipynb`.\n",
    "\n",
    "## Bonus points I\n",
    "\n",
    "Refactor the Summarize node into two distinct nodes, one for explanation, and one for joke rating\n",
    "\n",
    "## Bonus points II\n",
    "\n",
    "As a final step, plot the joke ratings in a Plotly chart, and have it displayed in Kedro Viz!\n",
    "\n",
    "## Bonus points III\n",
    "\n",
    "Come up with and implement more uses of LLMs to augment the rows in the given dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro_with_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
